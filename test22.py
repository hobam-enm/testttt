# -*- coding: utf-8 -*-
# ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„ê¸° â€” ìˆœìˆ˜ ì±—ë´‡ ëª¨ë“œ (ì„¸ì…˜ ê´€ë¦¬ ê¸°ëŠ¥ ìµœì¢… + URL ìˆ˜ì§‘ ì¼€ì´ìŠ¤ 1~3 ë°˜ì˜)
# [íŒ¨ì¹˜ ìš”ì•½]
# - ì—”í‹°í‹° ì œê±°: ë©”ì¸ í‚¤ì›Œë“œ 1ê°œë§Œ ì¶”ì¶œ/ê²€ìƒ‰
# - ìµœì†Œ ì˜¤ë¥˜ ë°©ì§€: shutil.copyfileë¡œ ë³µì‚¬(ìœˆë„ìš° í˜¸í™˜), ISO íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìµœê·¼ 7ì¼ ìë™ ì ìš©
# - ì‹œê°í™” ì´ì‹: ë‹¤ìš´ë¡œë“œ ë¸”ë¡ ë°”ë¡œ ì•„ë˜ì— ì •ëŸ‰ ì‹œê°í™”(í‚¤ì›Œë“œ ë²„ë¸”, ì‹œê³„ì—´, Top10 ë“±) ë…¸ì¶œ

import streamlit as st
import pandas as pd
import os
import re
import gc
import time
import json
import base64
import requests
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from uuid import uuid4
import io
import shutil  # âœ… ìµœì†Œ íŒ¨ì¹˜: Windows í˜¸í™˜ íŒŒì¼ ë³µì‚¬

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
from streamlit.components.v1 import html as st_html

# === ì‹œê°í™” ì˜ì¡´ì„± (ì—†ì–´ë„ ì•±ì´ ê¹¨ì§€ì§€ ì•Šê²Œ ê°€ë“œ) ===
try:
    import plotly.express as px
    from plotly import graph_objects as go
    import circlify
    import stopwordsiso as stopwords
    from kiwipiepy import Kiwi
    import numpy as np
    _VIS_READY = True
except Exception:
    _VIS_READY = False
    Kiwi = None

# ==============================================================================
# í˜ì´ì§€/ì „ì—­ ì„¤ì •
# ==============================================================================
st.set_page_config(
    page_title="ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„: ì±—ë´‡",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown(
    """
<style>
  /* Streamlit ë©”ì¸ ì»¨í…Œì´ë„ˆ íŒ¨ë”© ìµœì†Œí™” */
  .main .block-container {
      padding-top: 2rem;
      padding-right: 1rem;
      padding-left: 1rem;
      padding-bottom: 5rem;
  }
  [data-testid="stSidebarContent"] {
      padding-top: 1.5rem;
  }
  header {visibility: hidden;}
  footer {visibility: hidden;}
  #MainMenu {visibility: hidden;}

  /* --- ì‚¬ì´ë“œë°” ë„ˆë¹„ ê³ ì • --- */
  [data-testid="stSidebar"] {
      width: 350px !important;
      min-width: 350px !important;
      max-width: 350px !important;
  }
  [data-testid="stSidebar"] + div[class*="resizer"] {
      display: none;
  }

  /* AI ë‹µë³€ í°íŠ¸ í¬ê¸° ì¡°ì • */
  [data-testid="stChatMessage"]:has(span[data-testid="chat-avatar-assistant"]) p,
  [data-testid="stChatMessage"]:has(span[data-testid="chat-avatar-assistant"]) li {
      font-size: 0.95rem;
  }

  /* ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í…ìŠ¤íŠ¸ ë§í¬ì²˜ëŸ¼ ë³´ì´ê²Œ ìŠ¤íƒ€ì¼ë§ */
  .stDownloadButton button {
      background-color: transparent;
      color: #1c83e1;
      border: none;
      padding: 0;
      text-decoration: underline;
      font-size: 14px;
      font-weight: normal;
  }
  .stDownloadButton button:hover {
      color: #0b5cab;
  }

  /* ì„¸ì…˜ ëª©ë¡ ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
  .session-list .stButton button {
      font-size: 0.9rem;
      text-align: left;
      font-weight: normal;
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
      display: block;
  }

  /* ìƒˆ ì±„íŒ… ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
  .new-chat-btn button {
      background-color: #e8f0fe;
      color: #0052CC !important;
      border: 1px solid #d2e3fc !important;
  }
  .new-chat-btn button:hover {
      background-color: #d2e3fc;
      color: #0041A3 !important;
      border: 1px solid #c2d8f8 !important;
  }
</style>
""",
    unsafe_allow_html=True
)

# --- ê²½ë¡œ ë° GitHub ì„¤ì • ---
BASE_DIR = "/tmp"
SESS_DIR = os.path.join(BASE_DIR, "sessions")
os.makedirs(SESS_DIR, exist_ok=True)

GITHUB_TOKEN = st.secrets.get("GITHUB_TOKEN", "")
GITHUB_REPO = st.secrets.get("GITHUB_REPO", "")
GITHUB_BRANCH = st.secrets.get("GITHUB_BRANCH", "main")

KST = timezone(timedelta(hours=9))

def now_kst() -> datetime:
    return datetime.now(tz=KST)

def to_iso_kst(dt: datetime) -> str:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=KST)
    return dt.astimezone(KST).isoformat(timespec="seconds")

def kst_to_rfc3339_utc(dt_kst: datetime) -> str:
    if dt_kst.tzinfo is None:
        dt_kst = dt_kst.replace(tzinfo=KST)
    return dt_kst.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

# ==============================================================================
# í‚¤/ìƒìˆ˜
# ==============================================================================
_YT_FALLBACK, _GEM_FALLBACK = [], []
YT_API_KEYS       = list(st.secrets.get("YT_API_KEYS", [])) or _YT_FALLBACK
GEMINI_API_KEYS   = list(st.secrets.get("GEMINI_API_KEYS", [])) or _GEM_FALLBACK
GEMINI_MODEL      = "gemini-2.5-flash-lite"
GEMINI_TIMEOUT    = 120
GEMINI_MAX_TOKENS = 2048
MAX_TOTAL_COMMENTS   = 120_000
MAX_COMMENTS_PER_VID = 4_000

# ==============================================================================
# ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
# ==============================================================================
def ensure_state():
    defaults = {
        "chat": [],
        "last_schema": None,
        "last_csv": "",
        "last_df": None,
        "sample_text": "",
        "loaded_session_name": None
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

ensure_state()

# ==============================================================================
# GitHub API í•¨ìˆ˜
# ==============================================================================
def _gh_headers(token: str):
    return {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }

def github_upload_file(repo, branch, path_in_repo, local_path, token):
    url = f"https://api.github.com/repos/{repo}/contents/{path_in_repo}"
    with open(local_path, "rb") as f:
        content = base64.b64encode(f.read()).decode()

    headers = _gh_headers(token)
    get_resp = requests.get(url + f"?ref={branch}", headers=headers)
    sha = get_resp.json().get("sha") if get_resp.ok else None

    data = {
        "message": f"archive: {os.path.basename(path_in_repo)}",
        "content": content,
        "branch": branch
    }
    if sha:
        data["sha"] = sha

    resp = requests.put(url, headers=headers, json=data)
    resp.raise_for_status()
    return resp.json()

def github_list_dir(repo, branch, folder, token):
    url = f"https://api.github.com/repos/{repo}/contents/{folder}?ref={branch}"
    resp = requests.get(url, headers=_gh_headers(token))
    if resp.ok:
        return [item['name'] for item in resp.json() if item['type'] == 'dir']
    return []

def github_download_file(repo, branch, path_in_repo, token, local_path):
    url = f"https://api.github.com/repos/{repo}/contents/{path_in_repo}?ref={branch}"
    resp = requests.get(url, headers=_gh_headers(token))
    if resp.ok:
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, "wb") as f:
            f.write(base64.b64decode(resp.json()["content"]))
        return True
    return False

def github_delete_folder(repo, branch, folder_path, token):
    contents_url = f"https://api.github.com/repos/{repo}/contents/{folder_path}?ref={branch}"
    headers = _gh_headers(token)
    resp = requests.get(contents_url, headers=headers)
    if not resp.ok:
        return
    for item in resp.json():
        delete_url = f"https://api.github.com/repos/{repo}/contents/{item['path']}"
        data = {"message": f"delete: {item['name']}", "sha": item['sha'], "branch": branch}
        requests.delete(delete_url, headers=headers, json=data).raise_for_status()

def github_rename_session(old_name, new_name, token):
    contents_url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/sessions/{old_name}?ref={GITHUB_BRANCH}"
    resp = requests.get(contents_url, headers=_gh_headers(token))
    resp.raise_for_status()
    files_to_move = resp.json()

    for item in files_to_move:
        filename = item['name']
        local_path = os.path.join(SESS_DIR, filename)
        if not github_download_file(GITHUB_REPO, GITHUB_BRANCH, item['path'], token, local_path):
            raise Exception(f"Failed to download {filename} from {old_name}")
        github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{new_name}/{filename}", local_path, token)

    github_delete_folder(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{old_name}", token)

# ==============================================================================
# ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜
# ==============================================================================
def _build_session_name() -> str:
    if st.session_state.get("loaded_session_name"):
        return st.session_state.loaded_session_name

    schema = st.session_state.get("last_schema", {})
    kw = (schema.get("keywords", ["NoKeyword"]))[0]
    kw_slug = re.sub(r'[^\w-]', '', kw.replace(' ', '_'))[:20]
    return f"{kw_slug}_{now_kst().strftime('%Y-%m-%d_%H%M')}"

def save_current_session_to_github():
    if not all([GITHUB_REPO, GITHUB_TOKEN, st.session_state.chat, st.session_state.last_csv]):
        return False, "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ GitHub ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."

    sess_name = _build_session_name()
    local_dir = os.path.join(SESS_DIR, sess_name)
    os.makedirs(local_dir, exist_ok=True)

    try:
        meta_path = os.path.join(local_dir, "qa.json")
        meta_data = {
            "chat": st.session_state.chat,
            "last_schema": st.session_state.last_schema,
            "sample_text": st.session_state.sample_text
        }
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)

        comments_path = os.path.join(local_dir, "comments.csv")
        videos_path = os.path.join(local_dir, "videos.csv")

        # âœ… ìµœì†Œ íŒ¨ì¹˜: Windows í˜¸í™˜ ë³µì‚¬
        shutil.copyfile(st.session_state.last_csv, comments_path)
        if st.session_state.last_df is not None:
            st.session_state.last_df.to_csv(videos_path, index=False, encoding="utf-8-sig")

        github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/qa.json", meta_path, GITHUB_TOKEN)
        github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/comments.csv", comments_path, GITHUB_TOKEN)
        if os.path.exists(videos_path):
            github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/videos.csv", videos_path, GITHUB_TOKEN)

        st.session_state.loaded_session_name = sess_name
        return True, sess_name

    except Exception as e:
        return False, f"ì €ì¥ ì‹¤íŒ¨: {e}"

def load_session_from_github(sess_name: str):
    with st.spinner(f"ì„¸ì…˜ '{sess_name}' ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        try:
            local_dir = os.path.join(SESS_DIR, sess_name)

            qa_ok = github_download_file(
                GITHUB_REPO, GITHUB_BRANCH,
                f"sessions/{sess_name}/qa.json",
                GITHUB_TOKEN,
                os.path.join(local_dir, "qa.json")
            )
            comments_ok = github_download_file(
                GITHUB_REPO, GITHUB_BRANCH,
                f"sessions/{sess_name}/comments.csv",
                GITHUB_TOKEN,
                os.path.join(local_dir, "comments.csv")
            )
            videos_ok = github_download_file(
                GITHUB_REPO, GITHUB_BRANCH,
                f"sessions/{sess_name}/videos.csv",
                GITHUB_TOKEN,
                os.path.join(local_dir, "videos.csv")
            )

            if not (qa_ok and comments_ok):
                st.error("ì„¸ì…˜ í•µì‹¬ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return

            st.session_state.clear()
            ensure_state()

            with open(os.path.join(local_dir, "qa.json"), "r", encoding="utf-8") as f:
                meta = json.load(f)

            st.session_state.update({
                "chat": meta.get("chat", []),
                "last_schema": meta.get("last_schema", None),
                "last_csv": os.path.join(local_dir, "comments.csv"),
                "last_df": pd.read_csv(os.path.join(local_dir, "videos.csv"))
                           if videos_ok and os.path.exists(os.path.join(local_dir, "videos.csv"))
                           else pd.DataFrame(),
                "loaded_session_name": sess_name,
                "sample_text": meta.get("sample_text", "")
            })
        except Exception as e:
            st.error(f"ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì„¸ì…˜ ë¡œë“œ/ì‚­ì œ/ì´ë¦„ë³€ê²½ íŠ¸ë¦¬ê±° ì²˜ë¦¬
if 'session_to_load' in st.session_state:
    load_session_from_github(st.session_state.pop('session_to_load'))
    st.rerun()

if 'session_to_delete' in st.session_state:
    sess_name = st.session_state.pop('session_to_delete')
    with st.spinner(f"ì„¸ì…˜ '{sess_name}' ì‚­ì œ ì¤‘..."):
        github_delete_folder(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}", GITHUB_TOKEN)
    st.success("ì„¸ì…˜ ì‚­ì œ ì™„ë£Œ.")
    time.sleep(1)
    st.rerun()

if 'session_to_rename' in st.session_state:
    old, new = st.session_state.pop('session_to_rename')
    if old and new and old != new:
        with st.spinner("ì´ë¦„ ë³€ê²½ ì¤‘..."):
            try:
                github_rename_session(old, new, GITHUB_TOKEN)
                st.success("ì´ë¦„ ë³€ê²½ ì™„ë£Œ!")
            except Exception as e:
                st.error(f"ë³€ê²½ ì‹¤íŒ¨: {e}")
        time.sleep(1)
        st.rerun()

# ==============================================================================
# ì‚¬ì´ë“œë°” UI
# ==============================================================================
with st.sidebar:
    st.markdown(
        '<h2 style="font-weight:600; font-size:1.6rem; margin-bottom:1.5rem; '
        'background:-webkit-linear-gradient(45deg, #4285F4, #9B72CB, #D96570, #F2A60C); '
        '-webkit-background-clip:text; -webkit-text-fill-color:transparent;">'
        'ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„: AI ì±—ë´‡</h2>',
        unsafe_allow_html=True
    )
    st.caption("ë¬¸ì˜: ë¯¸ë””ì–´)ë””ì§€í„¸ë§ˆì¼€íŒ… ë°ì´í„°íŒŒíŠ¸")

    st.markdown(
        """<style>
        [data-testid="stSidebarUserContent"] {
            display: flex; flex-direction: column; height: calc(100vh - 4rem);
        }
        .sidebar-top-section { flex-grow: 1; overflow-y: auto; }
        .sidebar-bottom-section { flex-shrink: 0; }
        </style>""",
        unsafe_allow_html=True
    )

    st.markdown('<div class="sidebar-top-section">', unsafe_allow_html=True)

    st.markdown('<div class="new-chat-btn">', unsafe_allow_html=True)
    if st.button("âœ¨ ìƒˆ ì±„íŒ…", use_container_width=True):
        st.session_state.clear()
        st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)

    if st.session_state.chat and st.session_state.last_csv:
        if st.button("ğŸ’¾ í˜„ì¬ ëŒ€í™” ì €ì¥", use_container_width=True):
            with st.spinner("ì„¸ì…˜ ì €ì¥ ì¤‘..."):
                success, result = save_current_session_to_github()
            if success:
                st.success(f"'{result}' ì €ì¥ ì™„ë£Œ!")
                time.sleep(2)
                st.rerun()
            else:
                st.error(result)

    st.markdown("---")
    st.markdown("#### ëŒ€í™” ê¸°ë¡")

    if not all([GITHUB_TOKEN, GITHUB_REPO]):
        st.caption("GitHub ì„¤ì •ì´ Secretsì— ì—†ìŠµë‹ˆë‹¤.")
    else:
        try:
            sessions = sorted(
                github_list_dir(GITHUB_REPO, GITHUB_BRANCH, "sessions", GITHUB_TOKEN),
                reverse=True
            )
            if not sessions:
                st.caption("ì €ì¥ëœ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                editing_session = st.session_state.get("editing_session", None)
                st.markdown('<div class="session-list">', unsafe_allow_html=True)

                for sess in sessions:
                    if sess == editing_session:
                        new_name = st.text_input("ìƒˆ ì´ë¦„:", value=sess, key=f"new_name_{sess}")
                        c1, c2 = st.columns(2)
                        if c1.button("âœ…", key=f"save_{sess}"):
                            st.session_state.session_to_rename = (sess, new_name)
                            st.session_state.pop('editing_session', None)
                            st.rerun()
                        if c2.button("âŒ", key=f"cancel_{sess}"):
                            st.session_state.pop('editing_session', None)
                            st.rerun()
                    else:
                        c1, c2, c3 = st.columns([0.7, 0.15, 0.15])
                        if c1.button(sess, key=f"sess_{sess}", use_container_width=True):
                            st.session_state.session_to_load = sess
                            st.rerun()
                        if c2.button("âœï¸", key=f"edit_{sess}"):
                            st.session_state.editing_session = sess
                            st.rerun()
                        if c3.button("ğŸ—‘ï¸", key=f"del_{sess}"):
                            st.session_state.session_to_delete = sess
                            st.rerun()

                st.markdown('</div>', unsafe_allow_html=True)

        except Exception:
            st.error("ê¸°ë¡ ë¡œë”© ì‹¤íŒ¨")

    st.markdown('</div>', unsafe_allow_html=True)

    st.markdown('<div class="sidebar-bottom-section">', unsafe_allow_html=True)
    st.markdown("""<hr><h3>ğŸ“ ë¬¸ì˜</h3><p>ë¯¸ë””ì–´)ë””ì§€í„¸ë§ˆì¼€íŒ… ë°ì´í„°íŒŒíŠ¸</p>""", unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)

# ==============================================================================
# ë¡œì§ (í•µì‹¬ í•¨ìˆ˜ ì •ì˜)
# ==============================================================================
def scroll_to_bottom():
    st_html(
        "<script> "
        "let last_message = document.querySelectorAll('.stChatMessage'); "
        "if (last_message.length > 0) { "
        "  last_message[last_message.length - 1].scrollIntoView({behavior: 'smooth'}); "
        "} "
        "</script>",
        height=0
    )

# ---------- ì‹œê°í™” ìœ í‹¸/í•¨ìˆ˜(ì´ì‹) ----------
def clean_illegal(val):
    try:
        from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
        if isinstance(val, str):
            return ILLEGAL_CHARACTERS_RE.sub('', val)
    except Exception:
        pass
    return val

def clean_df_strings(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    obj_cols = df.select_dtypes(include=["object"]).columns.tolist()
    if not obj_cols: return df
    df2 = df.copy()
    for c in obj_cols:
        df2[c] = df2[c].map(clean_illegal)
    return df2

@st.cache_data(ttl=600, show_spinner=False)
def compute_keyword_counter_from_file(csv_path: str, stopset_list: list[str], per_comment_cap: int = 200) -> list[tuple[str,int]]:
    if not _VIS_READY or not csv_path or not os.path.exists(csv_path):
        return []
    from collections import Counter
    counter = Counter()
    try:
        kiwi = Kiwi()
    except Exception:
        return []
    stopset = set(stopset_list)
    for chunk in pd.read_csv(csv_path, usecols=["text"], chunksize=100_000):
        texts = (chunk["text"].astype(str).str.slice(0, per_comment_cap)).tolist()
        if not texts:
            continue
        tokens = kiwi.tokenize(" ".join(texts), normalize_coda=True)
        words = [t.form for t in tokens if t.tag in ("NNG","NNP") and len(t.form) > 1 and t.form not in stopset]
        counter.update(words)
    return counter.most_common(300)

def keyword_bubble_figure_from_counter(counter_items: list[tuple[str,int]]):
    if not _VIS_READY or not counter_items:
        return None
    df_kw = pd.DataFrame(counter_items[:30], columns=["word", "count"])
    df_kw["label"] = df_kw["word"] + "<br>" + df_kw["count"].astype(str)
    df_kw["scaled"] = np.sqrt(df_kw["count"])
    circles = circlify.circlify(
        [{"id": w, "datum": s} for w, s in zip(df_kw["word"], df_kw["scaled"])],
        show_enclosure=False,
        target_enclosure=circlify.Circle(x=0, y=0, r=1)
    )
    pos = {c.ex["id"]: (c.x, c.y, c.r) for c in circles if "id" in c.ex}
    df_kw["x"] = df_kw["word"].map(lambda w: pos[w][0])
    df_kw["y"] = df_kw["word"].map(lambda w: pos[w][1])
    df_kw["r"] = df_kw["word"].map(lambda w: pos[w][2])
    s_min, s_max = df_kw["scaled"].min(), df_kw["scaled"].max()
    df_kw["font_size"] = df_kw["scaled"].apply(lambda s: int(10 + (s - s_min) / max(s_max - s_min, 1) * 12))
    fig_kw = go.Figure()
    palette = px.colors.sequential.Blues
    df_kw["color_idx"] = df_kw["scaled"].apply(lambda s: int((s - s_min) / max(s_max - s_min, 1) * (len(palette) - 1)))
    for _, row in df_kw.iterrows():
        color = palette[int(row["color_idx"])]
        fig_kw.add_shape(type="circle", xref="x", yref="y",
                         x0=row["x"] - row["r"], y0=row["y"] - row["r"],
                         x1=row["x"] + row["r"], y1=row["y"] + row["r"],
                         line=dict(width=0), fillcolor=color, opacity=0.88, layer="below")
    fig_kw.add_trace(go.Scatter(
        x=df_kw["x"], y=df_kw["y"], mode="text",
        text=df_kw["label"], textposition="middle center",
        textfont=dict(color="white", size=df_kw["font_size"].tolist()),
        hovertext=df_kw["word"] + " (" + df_kw["count"].astype(str) + ")",
        hovertemplate="%{hovertext}<extra></extra>",
    ))
    fig_kw.update_xaxes(visible=False, range=[-1.05, 1.05])
    fig_kw.update_yaxes(visible=False, range=[-1.05, 1.05], scaleanchor="x", scaleratio=1)
    fig_kw.update_layout(title="Top30 í‚¤ì›Œë“œ ë²„ë¸”", plot_bgcolor="rgba(0,0,0,0)", margin=dict(l=0, r=0, t=40, b=0))
    return fig_kw

def timeseries_from_file(csv_path: str):
    if not _VIS_READY or not csv_path or not os.path.exists(csv_path):
        return None, None
    tmin = None; tmax = None
    for chunk in pd.read_csv(csv_path, usecols=["publishedAt"], chunksize=200_000):
        dt = pd.to_datetime(chunk["publishedAt"], errors="coerce", utc=True)
        if dt.notna().any():
            lo, hi = dt.min(), dt.max()
            tmin = lo if (tmin is None or (lo < tmin)) else tmin
            tmax = hi if (tmax is None or (hi > tmax)) else tmax
    if tmin is None or tmax is None:
        return None, None
    span_hours = (tmax - tmin).total_seconds()/3600.0
    use_hour = (span_hours <= 48)

    agg = {}
    for chunk in pd.read_csv(csv_path, usecols=["publishedAt"], chunksize=200_000):
        dt = pd.to_datetime(chunk["publishedAt"], errors="coerce", utc=True).dt.tz_convert("Asia/Seoul")
        dt = dt.dropna()
        if dt.empty: continue
        bucket = (dt.dt.floor("H") if use_hour else dt.dt.floor("D"))
        vc = bucket.value_counts()
        for t, c in vc.items():
            agg[t] = agg.get(t, 0) + int(c)
    ts = pd.Series(agg).sort_index().rename("count").reset_index().rename(columns={"index":"bucket"})
    return ts, ("ì‹œê°„ë³„" if use_hour else "ì¼ìë³„")

def top_authors_from_file(csv_path: str, topn=10):
    if not _VIS_READY or not csv_path or not os.path.exists(csv_path):
        return None
    counts = {}
    for chunk in pd.read_csv(csv_path, usecols=["author"], chunksize=200_000):
        vc = chunk["author"].astype(str).value_counts()
        for k, v in vc.items():
            counts[k] = counts.get(k, 0) + int(v)
    if not counts: return None
    s = pd.Series(counts).sort_values(ascending=False).head(topn)
    return s.reset_index().rename(columns={"index": "author", 0: "count"}).rename(columns={"count": "count"})

def render_quant_viz_from_paths(comments_csv_path: str, df_stats: pd.DataFrame, scope_label="(KST ê¸°ì¤€)"):
    if not _VIS_READY or not comments_csv_path or not os.path.exists(comments_csv_path):
        return
    with st.container(border=True):
        st.subheader("ğŸ“Š ì •ëŸ‰ ìš”ì•½")

        col1, col2 = st.columns(2)
        with col1:
            with st.container(border=True):
                st.markdown("**â‘  í‚¤ì›Œë“œ ë²„ë¸”**")
                try:
                    custom_stopwords = {
                        "ì•„","íœ´","ì•„ì´êµ¬","ì•„ì´ì¿ ","ì•„ì´ê³ ","ì–´","ë‚˜","ìš°ë¦¬","ì €í¬","ë”°ë¼","ì˜í•´","ì„","ë¥¼",
                        "ì—","ì˜","ê°€","ìœ¼ë¡œ","ë¡œ","ì—ê²Œ","ë¿ì´ë‹¤","ì˜ê±°í•˜ì—¬","ê·¼ê±°í•˜ì—¬","ì…ê°í•˜ì—¬","ê¸°ì¤€ìœ¼ë¡œ",
                        "ê·¸ëƒ¥","ëŒ“ê¸€","ì˜ìƒ","ì˜¤ëŠ˜","ì´ì œ","ë­","ì§„ì§œ","ì •ë§","ë¶€ë¶„","ìš”ì¦˜","ì œë°œ","ì™„ì „",
                        "ê·¸ê²Œ","ì¼ë‹¨","ëª¨ë“ ","ìœ„í•´","ëŒ€í•œ","ìˆì§€","ì´ìœ ","ê³„ì†","ì‹¤ì œ","ìœ íŠœë¸Œ","ì´ë²ˆ","ê°€ì¥","ë“œë¼ë§ˆ",
                    }
                    try:
                        korean_stopwords = stopwords.stopwords("ko")
                    except Exception:
                        korean_stopwords = set()
                    stopset = set(korean_stopwords); stopset.update(custom_stopwords)
                    main_kw = (st.session_state.get("last_schema", {}).get("keywords") or [""])[0]
                    if main_kw and Kiwi is not None:
                        tokens_q = Kiwi().tokenize(main_kw, normalize_coda=True)
                        query_words = [t.form for t in tokens_q if t.tag in ("NNG","NNP") and len(t.form) > 1]
                        stopset.update(query_words)
                    with st.spinner("í‚¤ì›Œë“œ ê³„ì‚° ì¤‘â€¦"):
                        items = compute_keyword_counter_from_file(comments_csv_path, list(stopset), per_comment_cap=200)
                    fig = keyword_bubble_figure_from_counter(items)
                    if fig is None:
                        st.info("í‘œì‹œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.info(f"í‚¤ì›Œë“œ ë¶„ì„ ë¶ˆê°€: {e}")

        with col2:
            with st.container(border=True):
                st.markdown("**â‘¡ ì‹œì ë³„ ëŒ“ê¸€ëŸ‰ ë³€ë™ ì¶”ì´**")
                ts, label = timeseries_from_file(comments_csv_path)
                if ts is not None:
                    fig_ts = px.line(ts, x="bucket", y="count", markers=True, title=f"{label} ëŒ“ê¸€ëŸ‰ ì¶”ì´ {scope_label}")
                    st.plotly_chart(fig_ts, use_container_width=True)
                else:
                    st.info("ëŒ“ê¸€ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        if df_stats is not None and not df_stats.empty:
            col3, col4 = st.columns(2)
            with col3:
                with st.container(border=True):
                    st.markdown("**â‘¢ Top10 ì˜ìƒ ëŒ“ê¸€ìˆ˜**")
                    top_vids = df_stats.sort_values(by="commentCount", ascending=False).head(10).copy()
                    top_vids["title_short"] = top_vids["title"].apply(lambda t: t[:20] + "â€¦" if isinstance(t, str) and len(t) > 20 else t)
                    fig_vids = px.bar(top_vids, x="commentCount", y="title_short",
                                      orientation="h", text="commentCount", title="Top10 ì˜ìƒ ëŒ“ê¸€ìˆ˜")
                    st.plotly_chart(fig_vids, use_container_width=True)
            with col4:
                with st.container(border=True):
                    st.markdown("**â‘£ ëŒ“ê¸€ ì‘ì„±ì í™œë™ëŸ‰ Top10**")
                    ta = top_authors_from_file(comments_csv_path, topn=10)
                    if ta is not None and not ta.empty:
                        fig_auth = px.bar(ta, x="count", y="author", orientation="h", text="count", title="Top10 ëŒ“ê¸€ ì‘ì„±ì í™œë™ëŸ‰")
                        st.plotly_chart(fig_auth, use_container_width=True)
                    else:
                        st.info("ì‘ì„±ì ë°ì´í„° ì—†ìŒ")

        with st.container(border=True):
            st.markdown("**â‘¤ ëŒ“ê¸€ ì¢‹ì•„ìš” Top10**")
            best = []
            for chunk in pd.read_csv(comments_csv_path, usecols=["video_id","video_title","author","text","likeCount"], chunksize=200_000):
                chunk["likeCount"] = pd.to_numeric(chunk["likeCount"], errors="coerce").fillna(0).astype(int)
                best.append(chunk.sort_values("likeCount", ascending=False).head(10))
            if best:
                df_top = pd.concat(best).sort_values("likeCount", ascending=False).head(10)
                for _, row in df_top.iterrows():
                    url = f"https://www.youtube.com/watch?v={row['video_id']}"
                    st.markdown(
                        f"<div style='margin-bottom:15px;'>"
                        f"<b>{int(row['likeCount'])} ğŸ‘</b> â€” {row.get('author','')}<br>"
                        f"<span style='font-size:14px;'>â–¶ï¸ <a href='{url}' target='_blank' style='color:black; text-decoration:none;'>"
                        f"{str(row.get('video_title','(ì œëª©ì—†ìŒ)'))[:60]}</a></span><br>"
                        f"> {str(row.get('text',''))[:150]}{'â€¦' if len(str(row.get('text','')))>150 else ''}"
                        f"</div>", unsafe_allow_html=True
                    )

# ---------- ë©”íƒ€/ë‹¤ìš´ë¡œë“œ + (ì¶”ê°€) ì‹œê°í™” ----------
def render_metadata_and_downloads():
    if not (schema := st.session_state.get("last_schema")):
        return

    kw_main = schema.get("keywords", [])
    start_iso = schema.get('start_iso', '')
    end_iso = schema.get('end_iso', '')

    try:
        start_dt_str = datetime.fromisoformat(start_iso).astimezone(KST).strftime('%Y-%m-%d %H:%M')
        end_dt_str   = datetime.fromisoformat(end_iso).astimezone(KST).strftime('%Y-%m-%d %H:%M')
    except (ValueError, TypeError):
        start_dt_str = start_iso.split('T')[0] if start_iso else ""
        end_dt_str   = end_iso.split('T')[0] if end_iso else ""

    with st.container(border=True):
        st.markdown(
            f"""
            <div style="font-size:14px; color:#4b5563; line-height:1.8;">
              <span style='font-weight:600;'>í‚¤ì›Œë“œ:</span> {', '.join(kw_main) if kw_main else '(ì—†ìŒ)'}<br>
              <span style='font-weight:600;'>ê¸°ê°„:</span> {start_dt_str} ~ {end_dt_str} (KST)
            </div>
            """,
            unsafe_allow_html=True
        )

        csv_path  = st.session_state.get("last_csv")
        df_videos = st.session_state.get("last_df")

        if csv_path and os.path.exists(csv_path) and df_videos is not None and not df_videos.empty:
            with open(csv_path, "rb") as f:
                comment_csv_data = f.read()

            buffer = io.BytesIO()
            df_videos.to_csv(buffer, index=False, encoding="utf-8-sig")
            video_csv_data = buffer.getvalue()

            keywords_str = "_".join(kw_main).replace(" ", "_") if kw_main else "data"
            now_str = now_kst().strftime('%Y%m%d')

            col1, col2, col3, _ = st.columns([1.1, 1.2, 1.2, 6.5])
            col1.markdown(
                "<div style='font-size:14px; color:#4b5563; font-weight:600; padding-top:5px;'>ë‹¤ìš´ë¡œë“œ:</div>",
                unsafe_allow_html=True
            )
            with col2:
                st.download_button("ì „ì²´ëŒ“ê¸€", comment_csv_data, f"comments_{keywords_str}_{now_str}.csv", "text/csv")
            with col3:
                st.download_button("ì˜ìƒëª©ë¡", video_csv_data, f"videos_{keywords_str}_{now_str}.csv", "text/csv")

        # âœ… ì¶”ê°€: ë‹¤ìš´ë¡œë“œ ë¸”ë¡ í•˜ë‹¨ì— ì‹œê°í™” ë…¸ì¶œ
        try:
            if csv_path:
                render_quant_viz_from_paths(csv_path, df_videos, scope_label="(KST ê¸°ì¤€)")
        except Exception:
            pass

def render_chat():
    for msg in st.session_state.chat:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

class RotatingKeys:
    def __init__(self, keys, state_key: str, on_rotate=None):
        self.keys = [k.strip() for k in (keys or []) if isinstance(k, str) and k.strip()][:10]
        self.state_key = state_key
        self.on_rotate = on_rotate

        idx = st.session_state.get(state_key, 0)
        self.idx = 0 if not self.keys else (idx % len(self.keys))
        st.session_state[state_key] = self.idx

    def current(self):
        return self.keys[self.idx % len(self.keys)] if self.keys else None

    def rotate(self):
        if not self.keys:
            return
        self.idx = (self.idx + 1) % len(self.keys)
        st.session_state[self.state_key] = self.idx
        if callable(self.on_rotate):
            self.on_rotate(self.idx, self.current())

class RotatingYouTube:
    def __init__(self, keys, state_key="yt_key_idx"):
        self.rot = RotatingKeys(keys, state_key)
        self.service = None
        self._build()

    def _build(self):
        key = self.rot.current()
        if not key:
            raise RuntimeError("YouTube API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        self.service = build("youtube", "v3", developerKey=key)

    def execute(self, factory):
        try:
            return factory(self.service).execute()
        except HttpError as e:
            status = getattr(getattr(e, 'resp', None), 'status', None)
            msg = (getattr(e, 'content', b'').decode('utf-8', 'ignore') or '').lower()
            if status in (403, 429) and any(t in msg for t in ["quota", "rate", "limit"]) and len(YT_API_KEYS) > 1:
                self.rot.rotate()
                self._build()
                return factory(self.service).execute()
            raise

# ---------- í”„ë¡¬í”„íŠ¸/íŒŒì„œ (ì—”í‹°í‹° ì œê±° ë²„ì „) ----------
LIGHT_PROMPT = (
    f"ì—­í• : ìœ íŠœë¸Œ ëŒ“ê¸€ ë°˜ì‘ ë¶„ì„ê¸°ì˜ ìì—°ì–´ í•´ì„ê°€.\n"
    f"ëª©í‘œ: í•œêµ­ì–´ ì…ë ¥ì—ì„œ [ê¸°ê°„(KST)]ê³¼ [ë©”ì¸ í‚¤ì›Œë“œ(1ê°œ)]ì™€ [ì˜µì…˜]ì„ í•´ì„.\n"
    f"ê·œì¹™:\n"
    f"- ê¸°ê°„ì€ Asia/Seoul ê¸°ì¤€, ìƒëŒ€ê¸°ê°„ì˜ ì¢…ë£ŒëŠ” ì§€ê¸ˆ.\n"
    f"- 'í‚¤ì›Œë“œ'ëŠ” ê²€ìƒ‰ì— ì‚¬ìš©í•  ê°€ì¥ í•µì‹¬ ì£¼ì œ(í”„ë¡œê·¸ë¨, ë¸Œëœë“œ, ì¸ë¬¼ ë“±) **1ê°œë§Œ** ë°˜í™˜í•œë‹¤.\n"
    f"- ì˜µì…˜ íƒì§€: include_replies, channel_filter(any|official|unofficial), lang(ko|en|auto).\n\n"
    f"ì¶œë ¥(5ì¤„ ê³ ì •):\n"
    f"- í•œ ì¤„ ìš”ì•½: <ë¬¸ì¥>\n"
    f"- ê¸°ê°„(KST): <YYYY-MM-DDTHH:MM:SS+09:00> ~ <YYYY-MM-DDTHH:MM:SS+09:00>\n"
    f"- í‚¤ì›Œë“œ: [<í•µì‹¬ í‚¤ì›Œë“œ 1ê°œ>]\n"
    f"- ì˜µì…˜: {{ include_replies: true|false, channel_filter: \"any|official|unofficial\", lang: \"ko|en|auto\" }}\n"
    f"- ì›ë¬¸: {{USER_QUERY}}\n\n"
    f"í˜„ì¬ KST: {to_iso_kst(now_kst())}\n"
    f"ì…ë ¥:\n{{USER_QUERY}}"
)

def call_gemini_rotating(model_name, keys, system_instruction, user_payload,
                         timeout_s=120, max_tokens=2048) -> str:
    rk = RotatingKeys(keys, "gem_key_idx")
    if not rk.current():
        raise RuntimeError("Gemini API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    for _ in range(len(rk.keys) or 1):
        try:
            genai.configure(api_key=rk.current())
            model = genai.GenerativeModel(
                model_name,
                generation_config={"temperature": 0.2, "max_output_tokens": max_tokens}
            )
            resp = model.generate_content(
                [system_instruction, user_payload],
                request_options={"timeout": timeout_s}
            )
            if out := getattr(resp, "text", None):
                return out
            if c0 := (getattr(resp, "candidates", None) or [None])[0]:
                if p0 := (getattr(c0, "content", None) and getattr(c0.content, "parts", None) or [None])[0]:
                    if hasattr(p0, "text"):
                        return p0.text
            return ""

        except Exception as e:
            if "429" in str(e).lower() and len(rk.keys) > 1:
                rk.rotate()
                continue
            raise

    return ""

def parse_light_block_to_schema(light_text: str) -> dict:
    raw = (light_text or "").strip()

    m_time = re.search(r"ê¸°ê°„\(KST\)\s*:\s*([^~]+)~\s*([^\n]+)", raw)
    start_iso, end_iso = (m_time.group(1).strip(), m_time.group(2).strip()) if m_time else (None, None)

    m_kw = re.search(r"í‚¤ì›Œë“œ\s*:\s*\[(.*?)\]", raw, flags=re.DOTALL)
    keywords = [p.strip() for p in re.split(r"\s*,\s*", m_kw.group(1)) if p.strip()] if m_kw else []

    m_opt = re.search(r"ì˜µì…˜\s*:\s*\{(.*?)\}", raw, flags=re.DOTALL)
    options = {"include_replies": False, "channel_filter": "any", "lang": "auto"}
    if m_opt:
        blob = m_opt.group(1)
        if ir := re.search(r"include_replies\s*:\s*(true|false)", blob, re.I):
            options["include_replies"] = (ir.group(1).lower() == "true")
        if cf := re.search(r"channel_filter\s*:\s*\"(any|official|unofficial)\"", blob, re.I):
            options["channel_filter"] = cf.group(1)
        if lg := re.search(r"lang\s*:\s*\"(ko|en|auto)\"", blob, re.I):
            options["lang"] = lg.group(1)

    # âœ… ê¸°ê°„ íŒŒì‹± ì‹¤íŒ¨ â†’ ìµœê·¼ 7ì¼ ìë™ ì ìš©
    if not (start_iso and end_iso):
        end_dt = now_kst()
        start_dt = end_dt - timedelta(days=7)
        start_iso, end_iso = to_iso_kst(start_dt), to_iso_kst(end_dt)

    if not keywords:
        keywords = [m[0]] if (m := re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", raw)) else ["ìœ íŠœë¸Œ"]

    return {
        "start_iso": start_iso,
        "end_iso": end_iso,
        "keywords": keywords[:1],   # ë©”ì¸ í‚¤ì›Œë“œ 1ê°œë§Œ
        "entities": [],             # ì—”í‹°í‹° ì œê±°
        "options": options,
        "raw": raw
    }

# ---------- YouTube ìˆ˜ì§‘ ----------
def yt_search_videos(rt, keyword, max_results, order="relevance",
                     published_after=None, published_before=None):
    video_ids, token = [], None
    while len(video_ids) < max_results:
        params = {
            "q": keyword,
            "part": "id",
            "type": "video",
            "order": order,
            "maxResults": min(50, max_results - len(video_ids))
        }
        if published_after:
            params["publishedAfter"] = published_after
        if published_before:
            params["publishedBefore"] = published_before
        if token:
            params["pageToken"] = token

        resp = rt.execute(lambda s: s.search().list(**params))
        video_ids.extend(it["id"]["videoId"] for it in resp.get("items", [])
                         if it["id"]["videoId"] not in video_ids)
        if not (token := resp.get("nextPageToken")):
            break
        time.sleep(0.25)
    return video_ids

def yt_video_statistics(rt, video_ids):
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        if not batch:
            continue

        resp = rt.execute(
            lambda s: s.videos().list(part="statistics,snippet,contentDetails", id=",".join(batch))
        )
        for item in resp.get("items", []):
            stats = item.get("statistics", {})
            snip  = item.get("snippet", {})
            cont  = item.get("contentDetails", {})
            dur   = cont.get("duration", "")

            h = re.search(r"(\d+)H", dur)
            m = re.search(r"(\d+)M", dur)
            s = re.search(r"(\d+)S", dur)
            dur_sec = (int(h.group(1)) * 3600 if h else 0) \
                      + (int(m.group(1)) * 60 if m else 0) \
                      + (int(s.group(1)) if s else 0)

            vid_id = item.get("id")
            rows.append({
                "video_id": vid_id,
                "video_url": f"https://www.youtube.com/watch?v={vid_id}",
                "title": snip.get("title", ""),
                "channelTitle": snip.get("channelTitle", ""),
                "publishedAt": snip.get("publishedAt", ""),
                "duration": dur,
                "shortType": "Shorts" if dur_sec <= 60 else "Clip",
                "viewCount": int(stats.get("viewCount", 0) or 0),
                "likeCount": int(stats.get("likeCount", 0) or 0),
                "commentCount": int(stats.get("commentCount", 0) or 0)
            })
        time.sleep(0.25)
    return rows

def yt_all_replies(rt, parent_id, video_id, title="", short_type="Clip", cap=None):
    replies, token = [], None
    while not (cap is not None and len(replies) >= cap):
        try:
            resp = rt.execute(lambda s: s.comments().list(
                part="snippet",
                parentId=parent_id,
                maxResults=100,
                pageToken=token,
                textFormat="plainText"
            ))
        except HttpError:
            break

        for c in resp.get("items", []):
            sn = c["snippet"]
            replies.append({
                "video_id": video_id,
                "video_title": title,
                "shortType": short_type,
                "comment_id": c.get("id", ""),
                "parent_id": parent_id,
                "isReply": 1,
                "author": sn.get("authorDisplayName", ""),
                "text": sn.get("textDisplay", "") or "",
                "publishedAt": sn.get("publishedAt", ""),
                "likeCount": int(sn.get("likeCount", 0) or 0)
            })

        if not (token := resp.get("nextPageToken")):
            break
        time.sleep(0.2)

    return replies[:cap] if cap is not None else replies

def yt_all_comments_sync(rt, video_id, title="", short_type="Clip",
                         include_replies=True, max_per_video=None):
    rows, token = [], None
    while not (max_per_video is not None and len(rows) >= max_per_video):
        try:
            resp = rt.execute(lambda s: s.commentThreads().list(
                part="snippet,replies",
                videoId=video_id,
                maxResults=100,
                pageToken=token,
                textFormat="plainText"
            ))
        except HttpError:
            break

        for it in resp.get("items", []):
            top = it["snippet"]["topLevelComment"]["snippet"]
            thread_id = it["snippet"]["topLevelComment"]["id"]

            rows.append({
                "video_id": video_id,
                "video_title": title,
                "shortType": short_type,
                "comment_id": thread_id,
                "parent_id": "",
                "isReply": 0,
                "author": top.get("authorDisplayName", ""),
                "text": top.get("textDisplay", "") or "",
                "publishedAt": top.get("publishedAt", ""),
                "likeCount": int(top.get("likeCount", 0) or 0)
            })

            if include_replies and int(it["snippet"].get("totalReplyCount", 0) or 0) > 0:
                cap = None if max_per_video is None else max(0, max_per_video - len(rows))
                if cap == 0:
                    break
                rows.extend(yt_all_replies(rt, thread_id, video_id, title, short_type, cap=cap))

        if not (token := resp.get("nextPageToken")):
            break
        time.sleep(0.2)

    return rows[:max_per_video] if max_per_video is not None else rows

def parallel_collect_comments_streaming(video_list, rt_keys, include_replies,
                                        max_total_comments, max_per_video, prog_bar):
    out_csv = os.path.join(BASE_DIR, f"collect_{uuid4().hex}.csv")
    wrote_header, total_written, done, total_videos = False, 0, 0, len(video_list)

    with ThreadPoolExecutor(max_workers=3) as ex:
        futures = {
            ex.submit(
                yt_all_comments_sync,
                RotatingYouTube(rt_keys),
                v["video_id"],
                v.get("title", ""),
                v.get("shortType", "Clip"),
                include_replies,
                max_per_video
            ): v for v in video_list
        }

        for f in as_completed(futures):
            try:
                if comm := f.result():
                    dfc = pd.DataFrame(comm)
                    dfc = clean_df_strings(dfc)  # ğŸ”§ ë¬¸ìì—´ í´ë¦°(ë¶ˆë²•ë¬¸ì ì œê±°)
                    dfc.to_csv(
                        out_csv,
                        index=False,
                        mode="a" if wrote_header else "w",
                        header=not wrote_header,
                        encoding="utf-8-sig"
                    )
                    wrote_header = True
                    total_written += len(dfc)
            except Exception:
                pass

            done += 1
            prog_bar.progress(
                min(0.90, 0.50 + (done / total_videos) * 0.40 if total_videos > 0 else 0.50),
                text="ëŒ“ê¸€ ìˆ˜ì§‘ì¤‘â€¦"
            )
            if total_written >= max_total_comments:
                break

    return out_csv, total_written

def serialize_comments_for_llm_from_file(csv_path: str,
                                         max_chars_per_comment=280,
                                         max_total_chars=420_000):
    if not os.path.exists(csv_path):
        return "", 0, 0
    try:
        df_all = pd.read_csv(csv_path)
    except Exception:
        return "", 0, 0
    if df_all.empty:
        return "", 0, 0

    df_top_likes = df_all.sort_values("likeCount", ascending=False).head(1000)
    df_remaining = df_all.drop(df_top_likes.index)
    df_random = df_remaining.sample(n=min(1000, len(df_remaining))) if not df_remaining.empty else pd.DataFrame()

    df_sample = pd.concat([df_top_likes, df_random])
    lines, total_chars = [], 0

    for _, r in df_sample.iterrows():
        if total_chars >= max_total_chars:
            break

        text = str(r.get("text", "") or "").replace("\n", " ")
        prefix = f"[{'R' if int(r.get('isReply', 0)) == 1 else 'T'}|â™¥{int(r.get('likeCount', 0))}] "
        prefix += f"{str(r.get('author', '')).replace('\n', ' ')}: "
        body = text[:max_chars_per_comment] + 'â€¦' if len(text) > max_chars_per_comment else text

        line = prefix + body
        if total_chars + len(line) + 1 > max_total_chars:
            break

        lines.append(line)
        total_chars += len(line) + 1

    return "\n".join(lines), len(lines), total_chars

TITLE_LINE_RE = re.compile(r"^\s{0,3}#{1,6}\s+.*$")
HEADER_DUP_RE = re.compile(r"ìœ íŠœë¸Œ\s*ëŒ“ê¸€\s*ë¶„ì„.*", re.IGNORECASE)

def tidy_answer(md: str) -> str:
    if not md:
        return md
    lines = [
        line for line in md.splitlines()
        if not (TITLE_LINE_RE.match(line) or HEADER_DUP_RE.search(line))
    ]
    cleaned, prev_blank = [], False
    for l in lines:
        is_blank = not l.strip()
        if is_blank and prev_blank:
            continue
        cleaned.append(l)
        prev_blank = is_blank
    return "\n".join(cleaned).strip()

# NEW: URLì—ì„œ videoId ì¶”ì¶œ + ì…ë ¥ë¬¸ì—ì„œ URL ì œê±°
YTB_ID_RE = re.compile(r"[A-Za-z0-9_-]{11}")

def extract_video_ids_from_text(text: str) -> list:
    """ì‚¬ìš©ì ì…ë ¥ì—ì„œ YouTube ë™ì˜ìƒ URLì„ ì°¾ì•„ videoId ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜."""
    if not text:
        return []
    ids = set()

    # youtu.be/VIDEOID
    for m in re.finditer(r"https?://youtu\.be/([A-Za-z0-9_-]{11})", text):
        ids.add(m.group(1))

    # youtube.com/shorts/VIDEOID
    for m in re.finditer(r"https?://(?:www\.)?youtube\.com/shorts/([A-Za-z0-9_-]{11})", text):
        ids.add(m.group(1))

    # youtube.com/watch?v=VIDEOID (& íŒŒë¼ë¯¸í„° í¬í•¨)
    for m in re.finditer(r"https?://(?:www\.)?youtube\.com/watch\?[^ \n]+", text):
        url = m.group(0)
        try:
            qs = dict((kv.split("=", 1) + [""])[:2] for kv in url.split("?", 1)[1].split("&"))
            v = qs.get("v", "")
            if YTB_ID_RE.fullmatch(v):
                ids.add(v)
        except Exception:
            pass

    # ê·¸ ì™¸ VIDEOID ë‹¨ë… ì¶”ì •ì€ ì•ˆì „ì„ ìœ„í•´ ë¯¸ì ìš©
    return list(ids)

def strip_urls(s: str) -> str:
    """ë¬¸ì¥ ë‚´ URLì„ ì œê±°í•´ ìì—°ì–´ë§Œ ë‚¨ê¹€(ê³µë°± ì •ë¦¬)."""
    if not s:
        return ""
    s = re.sub(r"https?://\S+", " ", s)
    return re.sub(r"\s+", " ", s).strip()

# âœ… ISO íŒŒì‹± ì•ˆì „ ë˜í¼
def _safe_iso_to_dt(s, default_dt):
    try:
        return datetime.fromisoformat(s)
    except Exception:
        return default_dt

# ==============================================================================
# íŒŒì´í”„ë¼ì¸
# ==============================================================================
def run_pipeline_first_turn(user_query: str,
                            extra_video_ids=None,
                            only_these_videos: bool = False):
    """
    - ìì—°ì–´ë§Œ: ê¸°ì¡´ ê²€ìƒ‰
    - ìì—°ì–´ + URL: ê¸°ì¡´ ê²€ìƒ‰ + extra_video_ids ì¶”ê°€
    - URLë§Œ: only_these_videos=True ë¡œ í•´ë‹¹ URL ëŒ“ê¸€ë§Œ ìˆ˜ì§‘
    """
    extra_video_ids = list(dict.fromkeys(extra_video_ids or []))

    prog_bar = st.progress(0, text="ì¤€ë¹„ ì¤‘â€¦")

    if not GEMINI_API_KEYS:
        return "ì˜¤ë¥˜: Gemini API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    prog_bar.progress(0.05, text="í•´ì„ì¤‘â€¦")
    light = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, "", LIGHT_PROMPT.replace("{USER_QUERY}", user_query))
    schema = parse_light_block_to_schema(light)
    st.session_state["last_schema"] = schema

    prog_bar.progress(0.10, text="ì˜ìƒ ìˆ˜ì§‘ì¤‘â€¦")
    if not YT_API_KEYS:
        return "ì˜¤ë¥˜: YouTube API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    rt = RotatingYouTube(YT_API_KEYS)

    # âœ… ISO ì‹¤íŒ¨ ì‹œ ìµœê·¼ 7ì¼ë¡œ í´ë°±
    start_dt = _safe_iso_to_dt(schema["start_iso"], now_kst() - timedelta(days=7))
    end_dt   = _safe_iso_to_dt(schema["end_iso"],   now_kst())

    kw_main  = schema.get("keywords", [])

    # --- ì˜ìƒ ID êµ¬ì„± (ë©”ì¸í‚¤ì›Œë“œ 1ê°œë§Œ) ---
    if only_these_videos and extra_video_ids:
        # 3) URLë§Œ ì „ë‹¬ â†’ í•´ë‹¹ URLë“¤ì˜ ëŒ“ê¸€ë§Œ ìˆ˜ì§‘
        all_ids = extra_video_ids
    else:
        # 1) ìì—°ì–´ë§Œ â†’ ë©”ì¸í‚¤ì›Œë“œ 1ê°œ ê²€ìƒ‰
        # 2) ìì—°ì–´ + URL â†’ ë©”ì¸í‚¤ì›Œë“œ ê²€ìƒ‰ + URL í•©ì‚°
        all_ids = []
        main_kw = (kw_main or ["ìœ íŠœë¸Œ"])[0]
        all_ids.extend(yt_search_videos(
            rt, main_kw, 60, "relevance",
            kst_to_rfc3339_utc(start_dt), kst_to_rfc3339_utc(end_dt)
        ))
        if extra_video_ids:
            all_ids.extend(extra_video_ids)

    all_ids = list(dict.fromkeys(all_ids))  # dedupe

    prog_bar.progress(0.40, text="ëŒ“ê¸€ ìˆ˜ì§‘ ì¤€ë¹„ì¤‘â€¦")

    df_stats = pd.DataFrame(yt_video_statistics(rt, all_ids))
    st.session_state["last_df"] = df_stats

    csv_path, total_cnt = parallel_collect_comments_streaming(
        df_stats.to_dict('records'),
        YT_API_KEYS,
        bool(schema.get("options", {}).get("include_replies")),
        MAX_TOTAL_COMMENTS,
        MAX_COMMENTS_PER_VID,
        prog_bar
    )
    st.session_state["last_csv"] = csv_path

    if total_cnt == 0:
        prog_bar.empty()
        return "ì§€ì • ì¡°ê±´ì—ì„œ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ ì‹œë„í•´ ë³´ì„¸ìš”."

    prog_bar.progress(0.90, text="AI ë¶„ì„ì¤‘â€¦")
    sample_text, _, _ = serialize_comments_for_llm_from_file(csv_path)
    st.session_state["sample_text"] = sample_text

    sys = (
        "ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë¨¼ì € [ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸]ì„ í™•ì¸í•˜ì—¬ "
        "ë¶„ì„ì˜ í•µì‹¬ ê´€ì (ì˜ˆ: íŠ¹ì • ì¸ë¬¼ ì¤‘ì‹¬, ê¸/ë¶€ì • ë°˜ì‘ ë“±)ì„ íŒŒì•…í•˜ë¼. "
        "ê·¸ ë‹¤ìŒ, ì£¼ì–´ì§„ ëŒ“ê¸€ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê´€ì ì— ë§ì¶° í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í•­ëª©í™”í•˜ê³ , "
        "ê¸/ë¶€/ì¤‘ ë¹„ìœ¨ê³¼ ëŒ€í‘œ ì½”ë©˜íŠ¸(10ê°œ ë¯¸ë§Œ)ë¥¼ ì œì‹œí•˜ë¼. "
        "ë‹¨, ì ˆëŒ€ë¡œ ë™ì¼í•œ ë‚´ìš©ì´ë‚˜ ë¬¸êµ¬ë¥¼ ë°˜ë³µí•´ì„œ ì¶œë ¥í•´ì„œëŠ” ì•ˆ ëœë‹¤."
    )
    payload = (
        f"[ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸]: {user_query}\n\n"
        f"[í‚¤ì›Œë“œ]: {', '.join(kw_main)}\n"
        f"[ê¸°ê°„(KST)]: {schema['start_iso']} ~ {schema['end_iso']}\n\n"
        f"[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{sample_text}\n"
    )
    answer_md_raw = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, sys, payload)

    prog_bar.progress(1.0, text="ì™„ë£Œ")
    time.sleep(0.5)
    prog_bar.empty()
    gc.collect()

    return tidy_answer(answer_md_raw)

def run_followup_turn(user_query: str):
    if not (schema := st.session_state.get("last_schema")):
        return "ì˜¤ë¥˜: ì´ì „ ë¶„ì„ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ì±„íŒ…ì„ ì‹œì‘í•´ì£¼ì„¸ìš”."

    sample_text = st.session_state.get("sample_text", "")
    context = "\n".join(
        f"[ì´ì „ {'Q' if m['role'] == 'user' else 'A'}]: {m['content']}"
        for m in st.session_state["chat"][-10:]
    )

    sys = (
        "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ í•µì‹¬ë§Œ ë‹µë³€í•˜ëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡ì´ë‹¤.\n"
        "--- ì¤‘ìš” ê·œì¹™ ---\n"
        "1. **ì§ˆë¬¸ ì˜ë„ íŒŒì•…ì´ ìµœìš°ì„ **: ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ **'ë¬´ì—‡(What)/ì–´ë–»ê²Œ(How)'**ì— ëŒ€í•œ ë‚´ìš©(ì •ì„±)ì„ ë¬»ëŠ”ì§€, "
        "**'ëª‡ ê°œ/ë¹„ìœ¨(How many)'**ì— ëŒ€í•œ ìˆ˜ì¹˜(ì •ëŸ‰)ë¥¼ ë¬»ëŠ”ì§€ ë¨¼ì € ëª…í™•íˆ êµ¬ë¶„í•˜ë¼.\n\n"
        "2. **'ë‚´ìš©(ì •ì„±)'ì„ ë¬¼ì—ˆì„ ë•Œ**: 'ì—°ê¸°ë ¥ ì–´ë•Œ?', 'ì–´ë–¤ ë‚´ìš©ì´ì•¼?' ì™€ ê°™ì€ ì§ˆë¬¸ì—ëŠ” **ì ˆëŒ€ ìˆ«ìë¡œë§Œ ë‹µí•˜ì§€ ë§ˆë¼.** "
        "ë°˜ë“œì‹œ ê´€ë ¨ëœ ì‹¤ì œ ëŒ“ê¸€ ë‚´ìš©ì„ ê·¼ê±°ë¡œ **í•µì‹¬ ë°˜ì‘ì„ ìš”ì•½**í•˜ê³ , **ì£¼ìš” ëŒ“ê¸€ì„ 1~3ê°œ ì¸ìš©**í•˜ë¼.\n\n"
        "3. **'ìˆ˜ì¹˜(ì •ëŸ‰)'ë¥¼ ë¬¼ì—ˆì„ ë•Œ**: í‚¤ì›Œë“œ ì–¸ê¸‰ íšŸìˆ˜ë¥¼ ì§ì ‘ ì„¸ì–´ **ìˆ«ì ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ** ë‹µí•˜ë¼.\n\n"
        "4. **ë™ë¬¸ì„œë‹µ ê¸ˆì§€**\n"
        "5. **'ëŒ“ê¸€ ìƒ˜í”Œ' ì§ì ‘ ì–¸ê¸‰ ê¸ˆì§€**\n"
        "6. **ë°˜ë³µ ê¸ˆì§€1/2**\n"
        "7. **ì œì™¸ í™•ì‹¤íˆ**: ì‚¬ìš©ìê°€ ì œì™¸ ìš”ì²­í•œ ë‚´ìš©ì€ ë¶„ì„ì—ì„œ ì œê±°í•˜ë¼."
    )

    payload = (
        f"{context}\n\n"
        f"[í˜„ì¬ ì§ˆë¬¸]: {user_query}\n"
        f"[ê¸°ê°„(KST)]: {schema.get('start_iso', '?')} ~ {schema.get('end_iso', '?')}\n\n"
        f"[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{sample_text}\n"
    )

    with st.spinner("ğŸ’¬ AIê°€ ë‹µë³€ì„ êµ¬ì„± ì¤‘ì…ë‹ˆë‹¤..."):
        response_raw = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, sys, payload)
        response = tidy_answer(response_raw)

    return response

# ==============================================================================
# ë©”ì¸ í™”ë©´ ë° ì‹¤í–‰ ë¡œì§
# ==============================================================================
if not st.session_state.chat:
    st.markdown(
        """
<div style="display:flex; flex-direction:column; align-items:center; justify-content:center;
            text-align:center; height:70vh;">
  <h1 style="font-size:3.5rem; font-weight:600;
             background:-webkit-linear-gradient(45deg, #4285F4, #9B72CB, #D96570, #F2A60C);
             -webkit-background-clip:text; -webkit-text-fill-color:transparent;">
    ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„: AI ì±—ë´‡
  </h1>
  <p style="font-size:1.2rem; color:#4b5563;">ê´€ë ¨ì˜ìƒ ìœ íŠœë¸Œ ëŒ“ê¸€ë°˜ì‘ì„ AIê°€ ìš”ì•½í•´ì¤ë‹ˆë‹¤</p>
  <div style="margin-top:3rem; padding:1rem 1.5rem; border:1px solid #e5e7eb; border-radius:12px;
              background-color:#fafafa; max-width:600px; text-align:left;">
    <h4 style="margin-bottom:1rem; font-weight:600;">âš ï¸ ì‚¬ìš© ì£¼ì˜ì‚¬í•­</h4>
    <ol style="padding-left:20px;">
      <li><strong>ì²« ì§ˆë¬¸ ì‹œ</strong> ëŒ“ê¸€ ìˆ˜ì§‘ ë° AI ë¶„ì„ì— ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
      <li>í•œ ì„¸ì…˜ì—ì„œëŠ” <strong>í•˜ë‚˜ì˜ ì£¼ì œ</strong>ë§Œ ì§„í–‰í•´ì•¼ ë¶„ì„ ì •í™•ë„ê°€ ìœ ì§€ë©ë‹ˆë‹¤.</li>
      <li>ì²« ì§ˆë¬¸ì—ëŠ” <strong>ê¸°ê°„ì„ ëª…ì‹œ</strong>í•´ì£¼ì„¸ìš” (ì˜ˆ: ìµœê·¼ 48ì‹œê°„ / 5ì›” 1ì¼ë¶€í„°).</li>
    </ol>
  </div>
</div>
""",
        unsafe_allow_html=True
    )
else:
    render_metadata_and_downloads()
    render_chat()
    scroll_to_bottom()

# ì…ë ¥ì°½
if prompt := st.chat_input("ì˜ˆ) ìµœê·¼ 24ì‹œê°„ íƒœí’ìƒì‚¬ ë°˜ì‘ ìš”ì•½í•´ì¤˜ / ë˜ëŠ” ì˜ìƒ URL ë¶™ì—¬ë„ OK"):
    st.session_state.chat.append({"role": "user", "content": prompt})
    st.rerun()

# ì…ë ¥ ì²˜ë¦¬
if st.session_state.chat and st.session_state.chat[-1]["role"] == "user":
    user_query = st.session_state.chat[-1]["content"]

    # URL/ìì—°ì–´ ë¶„ë¦¬
    url_ids = extract_video_ids_from_text(user_query)
    natural_text = strip_urls(user_query)
    has_urls = len(url_ids) > 0
    has_natural = len(natural_text) > 0

    # 3ê°€ì§€ ì¼€ì´ìŠ¤ ë¶„ê¸°
    if not st.session_state.get("last_csv"):
        # ì²« í„´: í•­ìƒ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
        if has_urls and not has_natural:
            # (3) URLë§Œ ì „ë‹¬ â†’ í•´ë‹¹ URLë“¤ì˜ ëŒ“ê¸€ë§Œ ìˆ˜ì§‘
            response = run_pipeline_first_turn(user_query, extra_video_ids=url_ids, only_these_videos=True)
        elif has_urls and has_natural:
            # (2) ìì—°ì–´ + URL â†’ ë©”ì¸í‚¤ì›Œë“œ ê²€ìƒ‰ + URL ì¶”ê°€ ìˆ˜ì§‘
            response = run_pipeline_first_turn(user_query, extra_video_ids=url_ids, only_these_videos=False)
        else:
            # (1) ìì—°ì–´ë§Œ â†’ ë©”ì¸í‚¤ì›Œë“œ 1ê°œ ê²€ìƒ‰
            response = run_pipeline_first_turn(user_query)
    else:
        # í›„ì† í„´: ê¸°ì¡´ ê·œì¹™ ìœ ì§€ (ì¶”ê°€ ìˆ˜ì§‘ ì—†ì´ ëŒ€í™” ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€)
        response = run_followup_turn(user_query)

    st.session_state.chat.append({"role": "assistant", "content": response})
    st.rerun()
